{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install datatable > /dev/null\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import datatable as dt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "#PurgedGroupTimeSeriesSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "#model\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tidalUtl.PrpUtl as prp\n",
    "import tidalUtl.EdaUtl as eda\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "#model save\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype.name\n",
    "\n",
    "        if col_type not in ['object', 'category', 'datetime64[ns, UTC]']:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ver1__<br>\n",
    "baseline：CV:0.01465 LB:0.01874<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"Xgboost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = \"/home/tidal/ML_Data/JaneStreet/jane-street-market-prediction\"\n",
    "OUTPUT = \"/home/tidal/ML_Data/JaneStreet/output\"\n",
    "#INPUT = \"/Users/hfuis/ML_Data/aneStreet/jane-street-market-prediction\"\n",
    "#OUTPUT = \"/Users/hfuis/ML_Data/aneStreet/jane-street-market-prediction\"\n",
    "\n",
    "SUBMIT = OUTPUT + \"/submittion/\"\n",
    "SAVEMODEL = OUTPUT + \"/model/\" + MODEL +\"/\"\n",
    "SAVEOOF = OUTPUT + \"/OOF/\" + MODEL +\"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2390491, 138)\n",
      "CPU times: user 25.3 s, sys: 1.8 s, total: 27.1 s\n",
      "Wall time: 2.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Loading\n",
    "train_data_datatable = dt.fread(INPUT + '/train.csv')\n",
    "trainFeature = train_data_datatable.to_pandas()\n",
    "print(trainFeature.shape)\n",
    "#trainFeature = trainFeature.query('weight > 0').pipe(reduce_mem_usage).reset_index(drop = True)\n",
    "#print(trainFeature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in trainFeature.columns if 'feature' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed固定\n",
    "def seed_everything(seed=42):\n",
    "    #data取得についてのランダム性固定\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    #torch.manual_seed(seed)\n",
    "    #torch.cuda.manual_seed(seed)\n",
    "    #cudnnによる演算の安定化(評価値の安定)\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    #os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HyperParameter\n",
    "param_space = {'n_estimators': 527,\n",
    "               'max_depth': 4,\n",
    "               'learning_rate': 0.03580866008035822,\n",
    "               'subsample': 0.5289264117776996,\n",
    "               'colsample_bytree': 0.8436545143704768,\n",
    "               'gamma': 5,\n",
    "               'missing': -999,\n",
    "               'tree_method': 'gpu_hist',\n",
    "               \"actThrs\":0.5,\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2390491 entries, 0 to 2390490\n",
      "Columns: 138 entries, date to ts_id\n",
      "dtypes: float64(135), int32(3)\n",
      "memory usage: 2.4 GB\n"
     ]
    }
   ],
   "source": [
    "trainFeature.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>weight</th>\n",
       "      <th>resp_1</th>\n",
       "      <th>resp_2</th>\n",
       "      <th>resp_3</th>\n",
       "      <th>resp_4</th>\n",
       "      <th>resp</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_121</th>\n",
       "      <th>feature_122</th>\n",
       "      <th>feature_123</th>\n",
       "      <th>feature_124</th>\n",
       "      <th>feature_125</th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "      <th>feature_128</th>\n",
       "      <th>feature_129</th>\n",
       "      <th>ts_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009916</td>\n",
       "      <td>0.014079</td>\n",
       "      <td>0.008773</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.006270</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.872746</td>\n",
       "      <td>-2.191242</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.168391</td>\n",
       "      <td>8.313583</td>\n",
       "      <td>1.782433</td>\n",
       "      <td>14.018213</td>\n",
       "      <td>2.653056</td>\n",
       "      <td>12.600292</td>\n",
       "      <td>2.301488</td>\n",
       "      <td>11.445807</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>16.673515</td>\n",
       "      <td>-0.002828</td>\n",
       "      <td>-0.003226</td>\n",
       "      <td>-0.007319</td>\n",
       "      <td>-0.011114</td>\n",
       "      <td>-0.009792</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.349537</td>\n",
       "      <td>-1.704709</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.178850</td>\n",
       "      <td>1.777472</td>\n",
       "      <td>-0.915458</td>\n",
       "      <td>2.831612</td>\n",
       "      <td>-1.417010</td>\n",
       "      <td>2.297459</td>\n",
       "      <td>-1.304614</td>\n",
       "      <td>1.898684</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025134</td>\n",
       "      <td>0.027607</td>\n",
       "      <td>0.033406</td>\n",
       "      <td>0.034380</td>\n",
       "      <td>0.023970</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.812780</td>\n",
       "      <td>-0.256156</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.115747</td>\n",
       "      <td>9.667908</td>\n",
       "      <td>5.542871</td>\n",
       "      <td>11.671595</td>\n",
       "      <td>7.281757</td>\n",
       "      <td>10.060014</td>\n",
       "      <td>6.638248</td>\n",
       "      <td>9.427299</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.004730</td>\n",
       "      <td>-0.003273</td>\n",
       "      <td>-0.000461</td>\n",
       "      <td>-0.000476</td>\n",
       "      <td>-0.003200</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.174378</td>\n",
       "      <td>0.344640</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.838853</td>\n",
       "      <td>0.499251</td>\n",
       "      <td>3.033732</td>\n",
       "      <td>1.513488</td>\n",
       "      <td>4.397532</td>\n",
       "      <td>1.266037</td>\n",
       "      <td>3.856384</td>\n",
       "      <td>1.013469</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.138531</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>-0.001215</td>\n",
       "      <td>-0.006219</td>\n",
       "      <td>-0.002604</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.172026</td>\n",
       "      <td>-3.093182</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.344850</td>\n",
       "      <td>4.101145</td>\n",
       "      <td>0.614252</td>\n",
       "      <td>6.623456</td>\n",
       "      <td>0.800129</td>\n",
       "      <td>5.233243</td>\n",
       "      <td>0.362636</td>\n",
       "      <td>3.926633</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390486</th>\n",
       "      <td>499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.005829</td>\n",
       "      <td>0.020342</td>\n",
       "      <td>0.015396</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.649365</td>\n",
       "      <td>-1.169996</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.896874</td>\n",
       "      <td>-1.260055</td>\n",
       "      <td>1.947725</td>\n",
       "      <td>-1.994399</td>\n",
       "      <td>-1.685163</td>\n",
       "      <td>-2.866165</td>\n",
       "      <td>-0.216130</td>\n",
       "      <td>-1.892048</td>\n",
       "      <td>0.901585</td>\n",
       "      <td>2390486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390487</th>\n",
       "      <td>499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-0.000935</td>\n",
       "      <td>-0.006326</td>\n",
       "      <td>-0.004718</td>\n",
       "      <td>1</td>\n",
       "      <td>2.432943</td>\n",
       "      <td>5.284504</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.936553</td>\n",
       "      <td>1.064936</td>\n",
       "      <td>3.119762</td>\n",
       "      <td>-0.419796</td>\n",
       "      <td>-0.208975</td>\n",
       "      <td>-0.146749</td>\n",
       "      <td>0.730166</td>\n",
       "      <td>0.648452</td>\n",
       "      <td>2.068737</td>\n",
       "      <td>2390487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390488</th>\n",
       "      <td>499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.007605</td>\n",
       "      <td>0.024907</td>\n",
       "      <td>0.016591</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.622475</td>\n",
       "      <td>-0.963682</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.956745</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>-2.279663</td>\n",
       "      <td>-0.950259</td>\n",
       "      <td>-4.388417</td>\n",
       "      <td>-1.669922</td>\n",
       "      <td>-3.288939</td>\n",
       "      <td>-1.336142</td>\n",
       "      <td>-2.814239</td>\n",
       "      <td>2390488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390489</th>\n",
       "      <td>499</td>\n",
       "      <td>0.283405</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>-0.001375</td>\n",
       "      <td>-0.003702</td>\n",
       "      <td>-0.002004</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.463757</td>\n",
       "      <td>-1.107228</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.035894</td>\n",
       "      <td>-1.780962</td>\n",
       "      <td>0.881246</td>\n",
       "      <td>-2.202140</td>\n",
       "      <td>-1.912601</td>\n",
       "      <td>-3.341684</td>\n",
       "      <td>-0.571188</td>\n",
       "      <td>-2.185795</td>\n",
       "      <td>0.627452</td>\n",
       "      <td>2390489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390490</th>\n",
       "      <td>499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001855</td>\n",
       "      <td>-0.001855</td>\n",
       "      <td>-0.001194</td>\n",
       "      <td>-0.000864</td>\n",
       "      <td>-0.001905</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.817184</td>\n",
       "      <td>-1.131577</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.571013</td>\n",
       "      <td>2.483421</td>\n",
       "      <td>8.284037</td>\n",
       "      <td>-0.698486</td>\n",
       "      <td>0.199953</td>\n",
       "      <td>-0.168395</td>\n",
       "      <td>2.051091</td>\n",
       "      <td>1.726072</td>\n",
       "      <td>5.823676</td>\n",
       "      <td>2390490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2390491 rows × 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     weight    resp_1    resp_2    resp_3    resp_4      resp  \\\n",
       "0           0   0.000000  0.009916  0.014079  0.008773  0.001390  0.006270   \n",
       "1           0  16.673515 -0.002828 -0.003226 -0.007319 -0.011114 -0.009792   \n",
       "2           0   0.000000  0.025134  0.027607  0.033406  0.034380  0.023970   \n",
       "3           0   0.000000 -0.004730 -0.003273 -0.000461 -0.000476 -0.003200   \n",
       "4           0   0.138531  0.001252  0.002165 -0.001215 -0.006219 -0.002604   \n",
       "...       ...        ...       ...       ...       ...       ...       ...   \n",
       "2390486   499   0.000000  0.000142  0.000142  0.005829  0.020342  0.015396   \n",
       "2390487   499   0.000000  0.000012  0.000012 -0.000935 -0.006326 -0.004718   \n",
       "2390488   499   0.000000  0.000499  0.000499  0.007605  0.024907  0.016591   \n",
       "2390489   499   0.283405 -0.000156 -0.000156 -0.001375 -0.003702 -0.002004   \n",
       "2390490   499   0.000000 -0.001855 -0.001855 -0.001194 -0.000864 -0.001905   \n",
       "\n",
       "         feature_0  feature_1  feature_2  ...  feature_121  feature_122  \\\n",
       "0                1  -1.872746  -2.191242  ...          NaN     1.168391   \n",
       "1               -1  -1.349537  -1.704709  ...          NaN    -1.178850   \n",
       "2               -1   0.812780  -0.256156  ...          NaN     6.115747   \n",
       "3               -1   1.174378   0.344640  ...          NaN     2.838853   \n",
       "4                1  -3.172026  -3.093182  ...          NaN     0.344850   \n",
       "...            ...        ...        ...  ...          ...          ...   \n",
       "2390486          1  -1.649365  -1.169996  ...    -1.896874    -1.260055   \n",
       "2390487          1   2.432943   5.284504  ...    -0.936553     1.064936   \n",
       "2390488          1  -0.622475  -0.963682  ...    -2.956745    -0.640334   \n",
       "2390489         -1  -1.463757  -1.107228  ...    -2.035894    -1.780962   \n",
       "2390490         -1  -1.817184  -1.131577  ...    -0.571013     2.483421   \n",
       "\n",
       "         feature_123  feature_124  feature_125  feature_126  feature_127  \\\n",
       "0           8.313583     1.782433    14.018213     2.653056    12.600292   \n",
       "1           1.777472    -0.915458     2.831612    -1.417010     2.297459   \n",
       "2           9.667908     5.542871    11.671595     7.281757    10.060014   \n",
       "3           0.499251     3.033732     1.513488     4.397532     1.266037   \n",
       "4           4.101145     0.614252     6.623456     0.800129     5.233243   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "2390486     1.947725    -1.994399    -1.685163    -2.866165    -0.216130   \n",
       "2390487     3.119762    -0.419796    -0.208975    -0.146749     0.730166   \n",
       "2390488    -2.279663    -0.950259    -4.388417    -1.669922    -3.288939   \n",
       "2390489     0.881246    -2.202140    -1.912601    -3.341684    -0.571188   \n",
       "2390490     8.284037    -0.698486     0.199953    -0.168395     2.051091   \n",
       "\n",
       "         feature_128  feature_129    ts_id  \n",
       "0           2.301488    11.445807        0  \n",
       "1          -1.304614     1.898684        1  \n",
       "2           6.638248     9.427299        2  \n",
       "3           3.856384     1.013469        3  \n",
       "4           0.362636     3.926633        4  \n",
       "...              ...          ...      ...  \n",
       "2390486    -1.892048     0.901585  2390486  \n",
       "2390487     0.648452     2.068737  2390487  \n",
       "2390488    -1.336142    -2.814239  2390488  \n",
       "2390489    -2.185795     0.627452  2390489  \n",
       "2390490     1.726072     5.823676  2390490  \n",
       "\n",
       "[2390491 rows x 138 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_0',\n",
       " 'feature_1',\n",
       " 'feature_2',\n",
       " 'feature_3',\n",
       " 'feature_4',\n",
       " 'feature_5',\n",
       " 'feature_6',\n",
       " 'feature_7',\n",
       " 'feature_8',\n",
       " 'feature_9',\n",
       " 'feature_10',\n",
       " 'feature_11',\n",
       " 'feature_12',\n",
       " 'feature_13',\n",
       " 'feature_14',\n",
       " 'feature_15',\n",
       " 'feature_16',\n",
       " 'feature_17',\n",
       " 'feature_18',\n",
       " 'feature_19',\n",
       " 'feature_20',\n",
       " 'feature_21',\n",
       " 'feature_22',\n",
       " 'feature_23',\n",
       " 'feature_24',\n",
       " 'feature_25',\n",
       " 'feature_26',\n",
       " 'feature_27',\n",
       " 'feature_28',\n",
       " 'feature_29',\n",
       " 'feature_30',\n",
       " 'feature_31',\n",
       " 'feature_32',\n",
       " 'feature_33',\n",
       " 'feature_34',\n",
       " 'feature_35',\n",
       " 'feature_36',\n",
       " 'feature_37',\n",
       " 'feature_38',\n",
       " 'feature_39',\n",
       " 'feature_40',\n",
       " 'feature_41',\n",
       " 'feature_42',\n",
       " 'feature_43',\n",
       " 'feature_44',\n",
       " 'feature_45',\n",
       " 'feature_46',\n",
       " 'feature_47',\n",
       " 'feature_48',\n",
       " 'feature_49',\n",
       " 'feature_50',\n",
       " 'feature_51',\n",
       " 'feature_52',\n",
       " 'feature_53',\n",
       " 'feature_54',\n",
       " 'feature_55',\n",
       " 'feature_56',\n",
       " 'feature_57',\n",
       " 'feature_58',\n",
       " 'feature_59',\n",
       " 'feature_60',\n",
       " 'feature_61',\n",
       " 'feature_62',\n",
       " 'feature_63',\n",
       " 'feature_64',\n",
       " 'feature_65',\n",
       " 'feature_66',\n",
       " 'feature_67',\n",
       " 'feature_68',\n",
       " 'feature_69',\n",
       " 'feature_70',\n",
       " 'feature_71',\n",
       " 'feature_72',\n",
       " 'feature_73',\n",
       " 'feature_74',\n",
       " 'feature_75',\n",
       " 'feature_76',\n",
       " 'feature_77',\n",
       " 'feature_78',\n",
       " 'feature_79',\n",
       " 'feature_80',\n",
       " 'feature_81',\n",
       " 'feature_82',\n",
       " 'feature_83',\n",
       " 'feature_84',\n",
       " 'feature_85',\n",
       " 'feature_86',\n",
       " 'feature_87',\n",
       " 'feature_88',\n",
       " 'feature_89',\n",
       " 'feature_90',\n",
       " 'feature_91',\n",
       " 'feature_92',\n",
       " 'feature_93',\n",
       " 'feature_94',\n",
       " 'feature_95',\n",
       " 'feature_96',\n",
       " 'feature_97',\n",
       " 'feature_98',\n",
       " 'feature_99',\n",
       " 'feature_100',\n",
       " 'feature_101',\n",
       " 'feature_102',\n",
       " 'feature_103',\n",
       " 'feature_104',\n",
       " 'feature_105',\n",
       " 'feature_106',\n",
       " 'feature_107',\n",
       " 'feature_108',\n",
       " 'feature_109',\n",
       " 'feature_110',\n",
       " 'feature_111',\n",
       " 'feature_112',\n",
       " 'feature_113',\n",
       " 'feature_114',\n",
       " 'feature_115',\n",
       " 'feature_116',\n",
       " 'feature_117',\n",
       " 'feature_118',\n",
       " 'feature_119',\n",
       " 'feature_120',\n",
       " 'feature_121',\n",
       " 'feature_122',\n",
       " 'feature_123',\n",
       " 'feature_124',\n",
       " 'feature_125',\n",
       " 'feature_126',\n",
       " 'feature_127',\n",
       " 'feature_128',\n",
       " 'feature_129']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Func: In & Out Type is DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FillNAN(trainFeature_inp):\n",
    "    trainFeature = trainFeature_inp.copy()\n",
    "    #trainFeature = trainFeature_inp\n",
    "    \n",
    "    \n",
    "    f_mean = trainFeature[features[1:]].mean()\n",
    "    trainFeature = trainFeature.query('weight > 0').pipe(reduce_mem_usage).reset_index(drop = True)\n",
    "    \n",
    "    \n",
    "    #欠損値を平均で埋める\n",
    "    trainFeature[features[1:]] = trainFeature[features[1:]].fillna(f_mean)\n",
    "    \n",
    "    #targetを生成\n",
    "    trainFeature['action'] = (trainFeature['resp'] > 0).astype('int')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return trainFeature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scaling(trainFeature_inp):\n",
    "    trainFeature = trainFeature_inp.copy()\n",
    "    #trainFeature = trainFeature_inp\n",
    "    \n",
    "    #Scaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(trainFeature[features])\n",
    "    \n",
    "    trainFeature[features] = scaler.transform(trainFeature[features])\n",
    "    \n",
    "    del scaler\n",
    "    \n",
    "    return trainFeature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__train,testにターゲット値も連結__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Collecting(trainFeature_inp):\n",
    "    trainFeature = trainFeature_inp.copy()\n",
    "    #trainFeature = trainFeature_inp\n",
    "    \n",
    "    #targetを生成\n",
    "    trainFeature['action'] = (trainFeature['resp'] > 0).astype('int')\n",
    "    \n",
    "    #target単体でも保持。\n",
    "    target = trainFeature['action'].copy()\n",
    "    target = pd.DataFrame(target,columns=[\"action\"])\n",
    "    return trainFeature, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(param, trainFeature):\n",
    "    \n",
    "    #欠損値処理\n",
    "    trainFeature = FillNAN(trainFeature)\n",
    "    \n",
    "    #standard scaler\n",
    "    trainFeature = Scaling(trainFeature)\n",
    "    \n",
    "    #trainにターゲット値を連結。\n",
    "    train, target = Collecting(trainFeature)\n",
    "    \n",
    "    \n",
    "    return train, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#trainVsl, targetVsl = preprocessing(param_space, trainFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainVsl.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#targetVsl.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#targetVsl.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(targetVsl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Train: \"+ str(trainVsl.shape))\n",
    "#print(\"Target: \"+ str(targetVsl.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#eda.chkDfIsNull(trainFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#eda.chkDfIsNull(trainVsl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config about Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configは辞書化しておく。\n",
    "def Config_about_Fitting(train, target):\n",
    "    confFitting = {}\n",
    "    \n",
    "    #Fitするときに\"y\"として使う列の列名配列\n",
    "    confFitting[\"target_cols\"] = target.columns.values.tolist()\n",
    "    #Fitするときに\"X\"として使う列の列名配列\n",
    "    #kfold, id等はここで削除。\n",
    "    #feature_cols = [c for c in train.columns if c not in confFitting[\"target_cols\"]]\n",
    "    #confFitting[\"feature_cols\"] = [c for c in feature_cols if c not in ['kfold']]\n",
    "    confFitting[\"feature_cols\"] = features\n",
    "    #特徴量、ターゲットのサイズ\n",
    "    confFitting[\"num_features\"]=len(confFitting[\"feature_cols\"])\n",
    "    confFitting[\"num_targets\"]=len(confFitting[\"target_cols\"])\n",
    "    \n",
    "    return confFitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss, Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss\n",
    "#なし。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metric\n",
    "#roc_auc_score(y_val, oof[te])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "#DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 4096\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NFOLDS = 5\n",
    "EARLY_STOPPING_STEPS = 7\n",
    "EARLY_STOP = True\n",
    "GROUP_GAP = 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_folds(train, target):\n",
    "    folds = train.copy()\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=NFOLDS)\n",
    "    \n",
    "    for f, (t_idx, v_idx) in enumerate(gkf.split(train, target, train[\"date\"])):\n",
    "        folds.loc[v_idx, 'kfold'] = int(f)\n",
    "        #print(type(v_idx))\n",
    "        #print(v_idx.shape)\n",
    "    \n",
    "    folds['kfold'] = folds['kfold'].astype(int)\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1->train, 0->valid, -1->使用なし\n",
    "def CV_folds_PurgedTimeSeries(train, target):\n",
    "    folds = train.copy()\n",
    "    \n",
    "    gkf = PurgedGroupTimeSeriesSplit(n_splits=NFOLDS, group_gap=GROUP_GAP)\n",
    "    #kf = PurgedGroupTimeSeriesSplit(\n",
    "    #   n_splits=NFOLDS,\n",
    "    #   max_train_group_size=150,\n",
    "    #   group_gap=20,\n",
    "    #   max_test_group_size=60\n",
    "    #\n",
    "    \n",
    "    for f, (t_idx, v_idx) in enumerate(gkf.split(train.values, target.values, train[\"date\"].values)):\n",
    "        \n",
    "        folds.loc[t_idx, f'kfold_{f}'] = 1\n",
    "        folds.loc[v_idx, f'kfold_{f}'] = 0\n",
    "        folds[f'kfold_{f}'] = folds[f'kfold_{f}'].fillna(-1)\n",
    "        folds[f'kfold_{f}'] = folds[f'kfold_{f}'].astype(int)\n",
    "        #print(\"train:\",len(t_idx),\"  valid:\",len(v_idx))\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 3.58 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##Preprocessing Data\n",
    "#trainVsl, targetVsl = preprocessing(param_space, trainFeature)\n",
    "##CV folds\n",
    "#foldsVsl= CV_folds_PurgedTimeSeries(trainVsl, targetVsl)\n",
    "#\n",
    "#foldsVsl.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#foldsVsl[\"kfold_0\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#foldsVsl[\"kfold_1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#foldsVsl[\"kfold_2\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#foldsVsl.query('date > 100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility_score_bincount(date, weight, resp, action):\n",
    "    #print(date.shape)\n",
    "    #print(weight.shape)\n",
    "    #print(resp.shape)\n",
    "    #print(action.shape)\n",
    "    #a = weight * resp * action\n",
    "    #print(a.shape)\n",
    "    \n",
    "    \n",
    "    count_i = len(np.unique(date))\n",
    "    Pi = np.bincount(date, weight * resp * action)\n",
    "    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n",
    "    u = np.clip(t, 0, 6) * np.sum(Pi)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_Evaluation(confFitting, oof, train, fold, param):\n",
    "    \n",
    "    #print(train[confFitting[\"target_cols\"]].shape)\n",
    "    #print(train.loc[:,confFitting[\"target_cols\"]].shape)\n",
    "    #print(len(oof))\n",
    "    #print(oof.shape)\n",
    "    train[confFitting[\"target_cols\"]] = oof\n",
    "    \n",
    "    #各引数の生成\n",
    "    date =  train[\"date\"].values\n",
    "    weight = train[\"weight\"].values\n",
    "    resp = train[\"resp\"].values\n",
    "    action = train[\"action\"].values\n",
    "    action = np.where(action >= param[\"actThrs\"], 1, 0).astype(int)\n",
    "    #print(action)\n",
    "    #utility scoreの計算。\n",
    "    score = utility_score_bincount(date, weight, resp, action)\n",
    "        \n",
    "    print(\"Fold\", fold, \" CV utility score: \", score)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Fold Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(confFitting, Tester, fold, seed, param,\n",
    "                 folds, train, target):\n",
    "    \n",
    "    seed_everything(seed)\n",
    "    \n",
    "    train = folds.copy()\n",
    "    \n",
    "    trn_idx = train[train[f'kfold_{fold}'] == 1].index\n",
    "    val_idx = train[train[f'kfold_{fold}'] == 0].index\n",
    "    \n",
    "    train_df = train[train[f'kfold_{fold}'] == 1].reset_index(drop=True)\n",
    "    valid_df = train[train[f'kfold_{fold}'] == 0].reset_index(drop=True)\n",
    "    \n",
    "    x_train, y_train  = train_df[confFitting[\"feature_cols\"]].values, train_df[confFitting[\"target_cols\"]].values\n",
    "    x_valid, y_valid =  valid_df[confFitting[\"feature_cols\"]].values, valid_df[confFitting[\"target_cols\"]].values\n",
    "    \n",
    "    params = {'colsample_bytree':  param[\"colsample_bytree\"],\n",
    "              'gamma': param[\"gamma\"],\n",
    "              'learning_rate': param[\"learning_rate\"],\n",
    "              'max_depth': param[\"max_depth\"],\n",
    "              'n_estimators': param[\"max_depth\"],\n",
    "              'subsample': param[\"subsample\"],\n",
    "              'tree_method':param[\"tree_method\"],\n",
    "              'missing': param[\"missing\"]\n",
    "             }\n",
    "    \n",
    "    model = XGBClassifier(**params)\n",
    "    \n",
    "    model.fit(x_train, y_train)\n",
    "    #print(\"y_train shape: \", y_train.shape)\n",
    "    #print(y_train[0:5,:])\n",
    "    #print(\"class: \", model.classes_)\n",
    "    \n",
    "    ### Save ###\n",
    "    dump(model, f\"{SAVEMODEL}SEED{seed}_FOLD{fold}.pth\")\n",
    "    \n",
    "    oof = np.zeros((len(train), target.iloc[:, :].shape[1]))\n",
    "    #oof = np.zeros(len(train))\n",
    "    tmp_oof = model.predict(x_valid)\n",
    "    print(tmp_oof)\n",
    "    tmp_oof = np.array([tmp_oof]).T\n",
    "    #tmp_np = np.array(tmp_oof)\n",
    "    print(\"tmp_oof shape: \", tmp_oof.shape)\n",
    "    #print(tmp_np[0:5,:])\n",
    "    #print(\"oof[val_idx].shape\", oof[val_idx].shape)\n",
    "    #print(\"np.array([tmp_oof[:,1]]).T\", np.array([tmp_oof[:,1]]).T.shape)\n",
    "    #print(\"np.array([tmp_oof[:,1]])\", np.array([tmp_oof[:,1]]).shape)\n",
    "    #print(\"oof[val_idx] zero\",oof[val_idx])\n",
    "    oof[val_idx] = tmp_oof\n",
    "    #print(\"oof[val_idx]\",oof[val_idx])\n",
    "    \n",
    "    #foldごとのCV evaluation(utility score)\n",
    "    score = CV_Evaluation(confFitting, tmp_oof, valid_df, fold, param)\n",
    "    \n",
    "    #foldごとのAUC\n",
    "    score_auc = roc_auc_score(y_valid, tmp_oof)\n",
    "    print(\"CV AUC score: \", score_auc)\n",
    "    \n",
    "    del train\n",
    "    \n",
    "    return oof\n",
    "    ################本コンペの仕様上テストデータがない#############################################\n",
    "    \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    #x_test = test[confFitting[\"feature_cols\"]].values\n",
    "    #testdataset = TestDataset(x_test)\n",
    "    #testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    #model = Model(\n",
    "    #    num_features=confFitting[\"num_features\"],\n",
    "    #    num_targets=confFitting[\"num_targets\"],\n",
    "    #    param=param\n",
    "    #)\n",
    "    #\n",
    "    #model.load_state_dict(torch.load(f\"{SAVEMODEL}SEED{seed}_FOLD{fold}.pth\"))\n",
    "    #model.to(DEVICE)\n",
    "    #\n",
    "    #predictions = np.zeros((len(test), target.iloc[:, 1:].shape[1]))\n",
    "    #predictions = inference_fn(model, testloader, DEVICE)\n",
    "    #\n",
    "    #\n",
    "    #return oof, predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold(Tester, NFOLDS, seed, param,\n",
    "              folds, train, target, confFitting):\n",
    "    oof = np.zeros((len(train), confFitting[\"num_targets\"]))\n",
    "    #predictions = np.zeros((len(test), confFitting[\"num_targets\"]))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        if Tester:\n",
    "            print('=' * 20, 'Fold', fold, '=' * 20)\n",
    "        oof_= run_training(confFitting, Tester, fold, seed, param,\n",
    "                                   folds, train, target)\n",
    "        \n",
    "        #predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    #return oof, predictions\n",
    "    return oof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特になし"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Submit(confFitting, predictions, test):\n",
    "    test[confFitting[\"target_cols\"]] = predictions\n",
    "    sub = sample_submission.drop(columns=confFitting[\"target_cols\"]).merge(test[['sig_id']+confFitting[\"target_cols\"]], on='sig_id', how='left').fillna(0)\n",
    "    sub.to_csv(f'{SUBMIT}submission.csv', index=False)\n",
    "\n",
    "    print(\"sub.shape\" + str(sub.shape))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Exec(param):\n",
    "    \n",
    "    #Tester(True/False)\n",
    "    Tester = True\n",
    "    \n",
    "    #Preprocessing Data\n",
    "    train, target = preprocessing(param_space, trainFeature)\n",
    "    \n",
    "    #CV folds\n",
    "    folds = CV_folds_PurgedTimeSeries(train, target)\n",
    "    \n",
    "    #Config about Fitting\n",
    "    confFitting = Config_about_Fitting(train, target)\n",
    "    \n",
    "    # Averaging on multiple SEEDS\n",
    "    #SEED = [0, 1, 2, 3 ,4, 5]\n",
    "    SEED = [42]\n",
    "    oof = np.zeros((len(train), confFitting[\"num_targets\"]))\n",
    "    #predictions = np.zeros((len(test), confFitting[\"num_targets\"]))\n",
    "    \n",
    "    ### RUN ###\n",
    "    for seed in SEED:\n",
    "        if Tester:\n",
    "            print('~' * 20, 'SEED', seed, '~' * 20)\n",
    "        oof_ = run_k_fold(Tester, NFOLDS, seed, param,\n",
    "                                       folds, train, target, confFitting)\n",
    "        oof += oof_ / len(SEED)\n",
    "        #predictions += predictions_ / len(SEED)\n",
    "    \n",
    "    #CV 評価\n",
    "    #utility\n",
    "    score_utl = CV_Evaluation(confFitting, oof, train, -1, param)\n",
    "    score_auc = roc_auc_score(target.values, oof)\n",
    "    print(\"CV AUC score: \", score_auc)\n",
    "    \n",
    "    \n",
    "    # 課題提出\n",
    "    #Submit(confFitting, predictions, test)\n",
    "    \n",
    "    #OOF save\n",
    "    np.save(SAVEOOF + 'oof', oof)\n",
    "    \n",
    "    return score_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 2078.45 MB\n",
      "Memory usage after optimization is: 540.40 MB\n",
      "Decreased by 74.0%\n",
      "~~~~~~~~~~~~~~~~~~~~ SEED 42 ~~~~~~~~~~~~~~~~~~~~\n",
      "==================== Fold 0 ====================\n",
      "[1 1 0 ... 1 1 1]\n",
      "tmp_oof shape:  (258754, 1)\n",
      "Fold 0  CV utility score:  412.23281271271253\n",
      "CV AUC score:  0.5119519786700366\n",
      "==================== Fold 1 ====================\n",
      "[1 1 1 ... 0 1 1]\n",
      "tmp_oof shape:  (295599, 1)\n",
      "Fold 1  CV utility score:  40.07107275899171\n",
      "CV AUC score:  0.5095271298771297\n",
      "==================== Fold 2 ====================\n",
      "[1 1 1 ... 1 1 1]\n",
      "tmp_oof shape:  (314367, 1)\n",
      "Fold 2  CV utility score:  683.3142440326053\n",
      "CV AUC score:  0.5144350316769792\n",
      "==================== Fold 3 ====================\n",
      "[1 1 1 ... 1 1 1]\n",
      "tmp_oof shape:  (341972, 1)\n",
      "Fold 3  CV utility score:  -0.0\n",
      "CV AUC score:  0.5064201856488102\n",
      "==================== Fold 4 ====================\n",
      "[1 1 1 ... 1 1 1]\n",
      "tmp_oof shape:  (370311, 1)\n",
      "Fold 4  CV utility score:  246.1495432344157\n",
      "CV AUC score:  0.5081047026676145\n",
      "Fold -1  CV utility score:  417.0619063627436\n",
      "CV AUC score:  0.5050432218859087\n",
      "CPU times: user 1min 26s, sys: 11.1 s, total: 1min 37s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "oof= Exec(param_space)\n",
    "#print(\"score: \" + str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predict(confFitting, param, test, target, fold, seed):\n",
    "    \n",
    "    seed_everything(seed)\n",
    "  \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    x_test = test[confFitting[\"feature_cols\"]].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=confFitting[\"num_features\"],\n",
    "        num_targets=confFitting[\"num_targets\"],\n",
    "        param=param\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"{SAVEMODEL}SEED{seed}_FOLD{fold}.pth\"))\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    predictions = np.zeros((len(test), target.iloc[:, 1:].shape[1]))\n",
    "    predictions = inference_fn(model, testloader, DEVICE)\n",
    "    \n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold_predict(confFitting, test, target, param, Tester, NFOLDS, seed):\n",
    "    predictions = np.zeros((len(test), confFitting[\"num_targets\"]))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        if Tester:\n",
    "            print('=' * 20, 'Fold', fold, '=' * 20)\n",
    "        pred_ = run_predict(confFitting, param, test, target, fold, seed)\n",
    "        \n",
    "        predictions += pred_ / NFOLDS\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SubmitPredict(confFitting, predictions, test, prefix):\n",
    "    test[confFitting[\"target_cols\"]] = predictions\n",
    "    sub = sample_submission.drop(columns=confFitting[\"target_cols\"]).merge(test[['sig_id']+confFitting[\"target_cols\"]], on='sig_id', how='left').fillna(0)\n",
    "    sub.to_csv(f'{SUBMIT}{prefix}submission.csv', index=False)\n",
    "\n",
    "    print(\"sub.shape\" + str(sub.shape))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(param):\n",
    "    #Tester(True/False)\n",
    "    Tester = False\n",
    "    \n",
    "    #Preprocessing Data\n",
    "    train, target = preprocessing(param_space, trainFeature)\n",
    "    \n",
    "    #CV folds\n",
    "    folds = CV_folds(train, target)\n",
    "    \n",
    "    #Config about Fitting\n",
    "    confFitting = Config_about_Fitting(train, test, target, folds)\n",
    "    \n",
    "    # Averaging on multiple SEEDS\n",
    "    SEED = [0, 1, 2, 3 ,4, 5]\n",
    "    predictions = np.zeros((len(test), confFitting[\"num_targets\"]))\n",
    "    \n",
    "    ### RUN ###\n",
    "    for seed in SEED:\n",
    "        if Tester:\n",
    "            print('~' * 20, 'SEED', seed, '~' * 20)\n",
    "        predictions_ = run_k_fold_predict(confFitting, test, target, param, Tester, NFOLDS, seed)\n",
    "        predictions += predictions_ / len(SEED)\n",
    "    \n",
    "    # 課題提出\n",
    "    prefix = \"\"\n",
    "    SubmitPredict(confFitting, predictions, test, prefix)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#Predict(param_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperopt\n",
    "from hyperopt import fmin, tpe, hp, rand, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#HyperParameter\n",
    "#param_space = {'hidden_size1': hp.choice('hidden_size1', [394, 512, 896, 1024]), \n",
    "#               'hidden_size2': hp.choice('hidden_size2', [394, 512, 896, 1024]), \n",
    "#               'hidden_size3': hp.choice('hidden_size3', [394, 512, 896, 1024]),\n",
    "#               'hidden_size4': hp.choice('hidden_size4', [394, 512, 896, 1024]),\n",
    "#               'hidden_size5': hp.choice('hidden_size5', [394, 512, 896, 1024]),\n",
    "#               'hidden_size6': hp.choice('hidden_size6', [394, 512, 896, 1024]),\n",
    "#               'dropOutRate0': hp.uniform('dropOutRate0', 0, 0.4), \n",
    "#               'dropOutRate1': hp.uniform('dropOutRate1', 0, 0.4),\n",
    "#               'dropOutRate2': hp.uniform('dropOutRate2', 0, 0.4),\n",
    "#               'dropOutRate3': hp.uniform('dropOutRate3', 0, 0.4),\n",
    "#               'dropOutRate4': hp.uniform('dropOutRate4', 0, 0.4),\n",
    "#               'dropOutRate5': hp.uniform('dropOutRate5', 0, 0.4),\n",
    "#               'dropOutRate6': hp.uniform('dropOutRate6', 0, 0.4),\n",
    "#               'smoothingRate': hp.loguniform('smoothingRate', -7, -2),\n",
    "#               'actThrs':hp.uniform('actThrs', 0, 1),\n",
    "#              }\n",
    "#\n",
    "#trials = Trials()\n",
    "#\n",
    "#hopt = fmin(fn = Exec, \n",
    "#            space = param_space, \n",
    "#            algo = tpe.suggest, \n",
    "#            max_evals = 25, \n",
    "#            #timeout = 8.9 * 60 * 60, \n",
    "#            trials = trials, \n",
    "#           )\n",
    "#\n",
    "#print(hopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
